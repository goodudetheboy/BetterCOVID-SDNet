{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487b3034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8004cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(directory:str, batch_size:int, test_size:int, rand_num:int, worker:int):\n",
    "    '''\n",
    "        directory: the directory of processed directory with class folders inside\n",
    "        batch_size: size of batch for training\n",
    "        test_size: percent of dataset used for test\n",
    "        rand_num: put random number for reproducibility\n",
    "        worker: number of worker in computation\n",
    "        \n",
    "        return train and test data ready for training\n",
    "    '''\n",
    "    #pipeline to resize images, crop, convert to tensor, and normalize\n",
    "    trans = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])])\n",
    "    \n",
    "    dataset = torchvision.datasets.ImageFolder(root=directory, transform=trans) #read image in folder to data with labels\n",
    "    \n",
    "    train_len = len(dataset) #get length of whole data\n",
    "    ind = list(range(train_len)) #indices of whole data\n",
    "    spl = int(np.floor(test_size * train_len)) #index of test data\n",
    "    \n",
    "    #reproducibility and shuffle step\n",
    "    np.random.seed(rand_num) \n",
    "    np.random.shuffle(ind)\n",
    "    \n",
    "    #sampling preparation steps\n",
    "    train_id, test_id = ind[spl:], ind[:spl]\n",
    "    tr_sampl = SubsetRandomSampler(train_id)\n",
    "    te_sampl = SubsetRandomSampler(test_id)\n",
    "\n",
    "    #use data loader to get train and test set ready for training\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=tr_sampl,num_workers=worker)\n",
    "    testloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, sampler=te_sampl,num_workers=worker)\n",
    "    return (trainloader, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e62057d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dire = \"./Data/Processed\" # directory of dataset\n",
    "# loading data loader\n",
    "trainloader, testloader = preprocess_data(directory=dire, batch_size=16, test_size=0.3, rand_num=40, worker=4)\n",
    "# getting device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73dd0eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sequential(\n",
      "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU(inplace=True)\n",
      "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU(inplace=True)\n",
      "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (6): ReLU(inplace=True)\n",
      "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU(inplace=True)\n",
      "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (11): ReLU(inplace=True)\n",
      "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (13): ReLU(inplace=True)\n",
      "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU(inplace=True)\n",
      "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (18): ReLU(inplace=True)\n",
      "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (20): ReLU(inplace=True)\n",
      "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU(inplace=True)\n",
      "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (25): ReLU(inplace=True)\n",
      "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU(inplace=True)\n",
      "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU(inplace=True)\n",
      "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "), AdaptiveAvgPool2d(output_size=(7, 7))]\n"
     ]
    }
   ],
   "source": [
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "modules = list(vgg16.children())[:-1]\n",
    "print(modules)\n",
    "vgg16_model = nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7edcc3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False), BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), ReLU(inplace=True), MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False), Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (4): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (5): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "), Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "), AdaptiveAvgPool2d(output_size=(1, 1))]\n"
     ]
    }
   ],
   "source": [
    "resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "modules = list(resnet50.children())[:-1]\n",
    "print(modules)\n",
    "resnet50_model = nn.Sequential(*modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2d06cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf0a2f81dadb41fa87562200eeb76644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:[1/10], training loss: 2.1596\n",
      "Epoch:[2/10], training loss: 1.3557\n",
      "Epoch:[3/10], training loss: 1.2582\n",
      "Epoch:[4/10], training loss: 1.1250\n",
      "Epoch:[5/10], training loss: 1.0117\n",
      "Epoch:[6/10], training loss: 1.0349\n",
      "Epoch:[7/10], training loss: 0.8174\n",
      "Epoch:[8/10], training loss: 0.7963\n",
      "Epoch:[9/10], training loss: 0.6547\n",
      "Epoch:[10/10], training loss: 0.5731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2909cc9b610>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#net = resnet50\n",
    "from ipywidgets import IntProgress\n",
    "net = vgg16\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net = net.to(device)\n",
    "losses = []\n",
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs)):  # loop over the dataset multiple times\n",
    "# YOUR CODE HERE\n",
    "    Loss = 0.0\n",
    "    count = 0\n",
    "    for iter, data in enumerate(trainloader, 0):\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        predicted = net(images)\n",
    "        _, pre1 = torch.max(predicted,dim=1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(predicted, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        Loss += loss.item() #accumulate the loss\n",
    "        count += 1\n",
    "\n",
    "    avg_loss = Loss/count\n",
    "    losses.append(avg_loss) #append the average loss for each batch\n",
    "    print('Epoch:[{}/{}], training loss: {:.4f}'.format(epoch+1, epochs, avg_loss))\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "591a4d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12.0579,  8.3368, 18.8805,  ..., -0.6232, -1.0158,  1.3395],\n",
      "        [12.5573, 12.8453,  9.3498,  ...,  0.2685, -0.5206, -0.5723],\n",
      "        [12.1452, 11.0017, 13.9310,  ...,  1.7481, -0.3436, -0.3170],\n",
      "        ...,\n",
      "        [12.0121, 13.5391, 10.9859,  ..., -1.0550, -1.1673,  0.6325],\n",
      "        [10.7536, 10.1124, 10.3241,  ...,  0.0449, -0.6461,  0.6054],\n",
      "        [13.1822,  9.5288, 18.1492,  ..., -0.4896,  0.1392,  1.4853]])\n",
      "tensor([[ 9.5027e+00,  8.4043e+00,  8.6727e+00,  ..., -4.7133e-01,\n",
      "         -1.8746e-01,  1.3598e-01],\n",
      "        [ 8.8426e+00,  7.1278e+00,  1.3988e+01,  ..., -5.3596e-01,\n",
      "         -1.6156e-01,  1.1526e+00],\n",
      "        [ 1.2928e+01,  1.2434e+01,  9.7942e+00,  ..., -3.0590e-01,\n",
      "         -3.3673e-01,  6.4226e-01],\n",
      "        ...,\n",
      "        [ 1.2059e+01,  8.2988e+00,  1.5801e+01,  ...,  3.7673e-01,\n",
      "         -3.0855e-01,  2.1624e-01],\n",
      "        [ 1.0676e+01,  8.6747e+00,  1.5129e+01,  ..., -2.3119e-01,\n",
      "         -1.0577e+00,  1.1558e+00],\n",
      "        [ 1.0197e+01,  7.2620e+00,  1.4772e+01,  ...,  6.1105e-01,\n",
      "         -3.6540e-03,  5.5119e-01]])\n",
      "tensor([[11.4017, 13.7093,  8.0091,  ..., -1.2534,  0.3307,  0.8455],\n",
      "        [ 9.8225,  9.7067, 13.1277,  ..., -0.3161, -0.3013,  0.6084],\n",
      "        [12.2979,  8.4542, 18.1261,  ..., -0.8036, -0.7407,  0.8539],\n",
      "        ...,\n",
      "        [ 9.4412,  6.0002, 13.2901,  ..., -0.3114,  0.0418,  0.6522],\n",
      "        [10.2501,  7.7687, 16.6153,  ...,  0.1317,  0.2200,  0.3849],\n",
      "        [17.2216, 18.6270, 12.8688,  ...,  0.3835,  0.2156,  1.1143]])\n",
      "tensor([[13.5016,  8.2335, 15.4697,  ...,  0.0483, -0.2899,  0.9230],\n",
      "        [13.1440, 10.3131, 14.8619,  ...,  1.1402,  0.1282,  0.9251],\n",
      "        [ 9.8949,  7.2295, 14.8381,  ..., -0.1146, -1.3325,  1.3739],\n",
      "        ...,\n",
      "        [10.2868,  8.9730, 14.7066,  ..., -0.2305, -0.9116,  1.2402],\n",
      "        [11.7238,  9.7023, 15.0582,  ..., -0.4925, -0.1667,  0.1226],\n",
      "        [ 9.3294, 12.5173,  8.3961,  ..., -1.4175, -0.2223,  1.0687]])\n",
      "tensor([[ 1.1190e+01,  8.6546e+00,  1.4705e+01,  ...,  1.8072e-01,\n",
      "         -8.8580e-03,  6.4153e-01],\n",
      "        [ 1.2086e+01,  9.7656e+00,  1.3150e+01,  ...,  2.7902e-01,\n",
      "         -3.8353e-01,  8.2190e-01],\n",
      "        [ 1.1040e+01,  7.4543e+00,  1.3622e+01,  ...,  6.1871e-01,\n",
      "         -5.6724e-01,  1.0911e+00],\n",
      "        ...,\n",
      "        [ 1.2593e+01,  1.1336e+01,  1.1123e+01,  ...,  2.8338e-01,\n",
      "         -1.7886e-01, -1.1627e-01],\n",
      "        [ 1.1204e+01,  1.2533e+01,  8.0074e+00,  ..., -2.2626e-01,\n",
      "          1.3165e-01, -2.5246e-01],\n",
      "        [ 1.2024e+01,  1.0413e+01,  1.3857e+01,  ...,  2.6952e-01,\n",
      "         -7.3208e-02,  2.1574e+00]])\n",
      "tensor([[ 1.0470e+01,  8.6462e+00,  1.5138e+01,  ..., -3.5165e-01,\n",
      "         -5.3084e-01,  5.7449e-01],\n",
      "        [ 1.2497e+01,  1.4340e+01,  1.1963e+01,  ...,  2.3993e-01,\n",
      "          2.1519e-01,  1.1580e+00],\n",
      "        [ 1.0102e+01,  6.1900e+00,  1.4275e+01,  ..., -1.8279e-01,\n",
      "         -3.7162e-02,  7.2685e-01],\n",
      "        ...,\n",
      "        [ 1.4475e+01,  1.3241e+01,  1.1333e+01,  ..., -7.1194e-01,\n",
      "          1.6364e+00, -5.2123e-01],\n",
      "        [ 1.1011e+01,  1.0534e+01,  1.4058e+01,  ...,  4.4774e-01,\n",
      "         -1.8104e-01,  1.3354e+00],\n",
      "        [ 1.4156e+01,  1.1803e+01,  1.3428e+01,  ...,  4.2385e-01,\n",
      "          5.4928e-02,  1.1146e-02]])\n",
      "tensor([[13.6290, 12.6113, 19.7888,  ..., -0.6758, -1.5259,  0.8879],\n",
      "        [12.0959,  9.8999, 15.6150,  ..., -0.4051, -0.2813,  0.6247],\n",
      "        [11.4392,  9.4357, 15.0080,  ..., -0.1167, -0.1754,  1.2459],\n",
      "        ...,\n",
      "        [13.7465, 11.0194, 14.3333,  ...,  0.3194, -0.3458,  0.6826],\n",
      "        [10.8689,  8.9309, 11.1466,  ...,  0.9196, -0.3038, -0.1138],\n",
      "        [12.2439, 14.3160, 10.0183,  ..., -1.4055, -0.1017,  0.8359]])\n",
      "tensor([[13.6670, 10.7114, 13.5704,  ...,  0.4376, -0.0358,  0.7497],\n",
      "        [11.4458, 11.2248,  9.4127,  ..., -0.8509,  0.0303,  0.1495],\n",
      "        [12.4519,  7.9451, 12.6491,  ...,  0.3123,  0.5639,  0.4633],\n",
      "        ...,\n",
      "        [10.0726,  7.0016, 11.7570,  ..., -0.1348,  0.0429,  0.8895],\n",
      "        [12.6090, 11.2788, 11.5062,  ...,  0.4495,  0.3359,  0.4370],\n",
      "        [10.4280,  9.4950, 10.4872,  ..., -0.0829,  0.3455,  0.3984]])\n",
      "tensor([[ 8.4947e+00,  1.0424e+01,  7.1309e+00,  ..., -5.1851e-01,\n",
      "          5.0050e-01,  3.9950e-01],\n",
      "        [ 1.0590e+01,  1.2880e+01,  8.6708e+00,  ..., -1.8384e+00,\n",
      "          3.4586e-01,  3.4138e-01],\n",
      "        [ 1.2331e+01,  9.2190e+00,  1.4490e+01,  ..., -1.5326e+00,\n",
      "         -1.6756e-01,  7.0027e-01],\n",
      "        ...,\n",
      "        [ 9.3165e+00,  6.0412e+00,  1.5303e+01,  ...,  8.7440e-02,\n",
      "         -7.8012e-01,  1.6570e+00],\n",
      "        [ 1.2860e+01,  8.2255e+00,  1.8049e+01,  ...,  9.5936e-03,\n",
      "         -3.1860e-01,  3.7999e-03],\n",
      "        [ 1.4883e+01,  1.3250e+01,  1.4568e+01,  ...,  2.0138e-01,\n",
      "         -1.8126e-01,  1.0190e+00]])\n",
      "tensor([[11.2847,  8.4753, 15.8477,  ..., -0.1511, -0.4778,  0.7833],\n",
      "        [10.6785,  8.3596, 10.8609,  ...,  0.9064, -0.2179,  0.4369],\n",
      "        [10.0308, 11.1722,  9.2169,  ..., -0.5864, -0.1599,  0.2635],\n",
      "        ...,\n",
      "        [ 7.5801, 10.0338,  6.6184,  ..., -0.7786, -1.1311,  1.1209],\n",
      "        [12.4282,  7.3593, 17.8746,  ..., -0.6741, -1.0153,  0.3598],\n",
      "        [12.3900,  8.7748, 17.0542,  ...,  0.3119, -0.7810,  0.7864]])\n",
      "tensor([[13.1078,  9.1877, 14.8787,  ...,  1.1153, -0.2285,  0.1150],\n",
      "        [ 9.4528,  8.1506, 11.2258,  ...,  0.5944, -0.5216,  0.7594],\n",
      "        [12.0163, 13.7395, 12.6111,  ...,  0.5700, -0.3458,  1.3692],\n",
      "        ...,\n",
      "        [13.5043, 14.3495, 10.0943,  ...,  0.0461,  0.4418,  0.2722],\n",
      "        [11.3032,  8.6185, 16.6747,  ..., -0.6922, -1.1167,  0.8841],\n",
      "        [13.5947, 12.3982, 15.6385,  ..., -0.7674, -1.1537,  0.8059]])\n",
      "tensor([[ 1.0475e+01,  8.3766e+00,  1.2963e+01,  ..., -4.3748e-01,\n",
      "         -4.9459e-01,  7.8238e-01],\n",
      "        [ 1.2404e+01,  1.1055e+01,  1.2563e+01,  ...,  1.0284e+00,\n",
      "         -3.6769e-01,  2.8495e-01],\n",
      "        [ 1.2373e+01,  9.9100e+00,  1.5924e+01,  ..., -1.6505e-01,\n",
      "         -3.2957e-01,  1.0890e+00],\n",
      "        ...,\n",
      "        [ 1.2223e+01,  1.6148e+01,  1.2757e+01,  ..., -1.4653e+00,\n",
      "         -7.7968e-01,  1.9464e+00],\n",
      "        [ 1.5576e+01,  1.2029e+01,  1.6352e+01,  ...,  2.0493e-01,\n",
      "         -1.5283e-01,  9.2848e-01],\n",
      "        [ 1.1229e+01,  8.3475e+00,  1.2268e+01,  ..., -1.5032e-02,\n",
      "          8.5864e-02,  2.2065e-01]])\n",
      "tensor([[13.5140,  9.6066, 16.4945,  ..., -0.1706, -0.3869,  0.8450],\n",
      "        [11.6037,  7.3636, 16.3032,  ...,  0.4551, -0.7468,  0.8446],\n",
      "        [11.6566,  9.2040, 12.2352,  ..., -0.0380, -0.0315,  0.7360],\n",
      "        ...,\n",
      "        [ 9.9105,  9.0495, 10.4279,  ..., -0.9368,  0.2082,  0.5422],\n",
      "        [12.9241, 10.0242, 12.2854,  ...,  0.3033, -0.5697,  0.2435],\n",
      "        [13.6659,  9.0082, 13.0671,  ...,  0.2235,  1.1209, -0.4006]])\n",
      "tensor([[10.0435,  6.2608, 16.7736,  ..., -0.2151, -0.9804,  1.1195],\n",
      "        [11.3666,  9.2924, 12.9647,  ...,  0.9680,  0.3310,  0.7333],\n",
      "        [10.1502,  5.5742, 11.7960,  ..., -0.8309,  0.4315,  0.5755],\n",
      "        ...,\n",
      "        [ 9.9916, 10.3963,  9.1495,  ..., -0.3475,  0.3363,  0.4916],\n",
      "        [11.6148,  9.2779, 11.6740,  ...,  0.0374, -0.4112,  0.8798],\n",
      "        [10.7584,  7.8152, 14.3196,  ...,  0.5310, -0.4591,  0.4483]])\n",
      "tensor([[ 6.9485,  9.6892,  7.7595,  ..., -1.4577, -1.2464,  0.8464],\n",
      "        [ 8.8625, 10.7550,  7.2567,  ..., -1.5476,  0.5829,  1.0008],\n",
      "        [ 9.7070,  8.1232, 11.1313,  ...,  0.0508, -0.5236,  1.4883],\n",
      "        ...,\n",
      "        [12.0303, 10.8676, 10.3969,  ...,  0.4264,  0.9931, -0.3219],\n",
      "        [13.0952, 11.8761, 12.4261,  ...,  0.3774,  0.4270,  0.3172],\n",
      "        [11.5881, 10.1301, 14.8991,  ..., -1.2742, -1.2433,  0.4397]])\n",
      "tensor([[ 8.3603, 12.1876,  6.9075,  ...,  0.0637, -0.3153,  2.0836],\n",
      "        [14.3528, 11.7107, 16.2174,  ...,  0.1466, -0.7566,  0.8970],\n",
      "        [12.4611,  9.3182, 17.0585,  ..., -1.2802, -0.5512,  1.5729],\n",
      "        ...,\n",
      "        [ 9.2689,  6.3973, 10.0889,  ..., -0.3291, -0.7198, -0.3946],\n",
      "        [13.0189, 11.3936, 14.9090,  ...,  0.1993,  0.3852,  0.7192],\n",
      "        [13.3913, 11.2149, 13.3907,  ..., -0.8650,  0.3441,  1.6780]])\n",
      "Accuracy of the network on the 10000 validation x: 59 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# YOUR CODE HERE\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        y_hat = net(images)\n",
    "        max_val, max_i = torch.max(y_hat.data, 1)\n",
    "        print(y_hat)\n",
    "        total += labels.size(0)\n",
    "        correct += (max_i == labels).sum().item()\n",
    "        \n",
    "print(f'Accuracy of the network on the 10000 validation x: {100 * correct // total} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
